{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TorchMX: PyTorch Quantization Framework For OCP MX Datatypes","text":"<p>This package a simulation tool implementing the MX quantization format specified in the OCP Micro Scaling Formats. The pacakage includes:</p> <ul> <li>Tensor subclasses for representing MX quantized data <code>MXTensor</code>.</li> <li>Quantization and dequantization functions for converting between high-precision and MX quantized tensors.</li> <li>Support for various MX data types, including FP8, FP6, FP4, and INT8.</li> <li>Custom <code>ATen</code> operations for <code>MXTensor</code></li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install torchmx\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Here's a basic example of how to quantize a PyTorch tensor to MX format:</p> <pre><code>import torch\nfrom torchmx import MXTensor, dtypes\n\n# Create a high-precision tensor\nx_hp = torch.randn(128, 128, dtype=torch.bfloat16)\n\n# Quantize the tensor to MX format\nx_mx = MXTensor.to_mx(x_hp, elem_dtype=dtypes.float8_e4m3, block_size=32)\n\n# Dequantize the tensor back to high-precision\nx_hp_reconstructed = x_mx.to_dtype(torch.bfloat16)\n\n# Matmul 2 MXTensors\ny_hp = torch.randn(128, 128, dtype=torch.bfloat16)\ny_mx = MXTensor.to_mx(y_mx, elem_dtype=dtypes.float6_e3m2, block_size=32)\n\n# Notice the magic here. You can pass MXTensors into torch.matmul.\n# This even works for 4D Attention Matmuls torch.matmul(Q, K.t).\n# The output is a bf16 torch.Tensor\nout_bf16 = torch.matmul(x_mx, y_mx)\n</code></pre>"},{"location":"#quantizing-layers-and-modules","title":"Quantizing Layers and Modules","text":"<p>TorchMX also provides tools for quantizing individual layers and modules. Here's an example of how to quantize all the linear layers in the model. The following example demonstrates how to quantize a model with torch.nn.Linear layers to MX format using the MXInferenceLinear class. In this case the weights are quantized <code>MX-fp6_e3m2</code> and the inputs to <code>MX-fp8_e4m3</code></p> <pre><code>from torch import nn\nfrom torchmx import MXTensor, dtypes\nfrom torchmx.config import QLinearConfig, MXConfig\nfrom torchmx.quant_api import quantize_linear_\n\n# Create a high-precision model\nmodel = nn.Sequential(\n          nn.Linear(128, 256),\n          nn.ReLU(),\n          nn.Linear(256, 64),\n          nn.ReLU()\n        ).to(torch.bfloat16)\n\n# Define the quantization configuration\nqconfig = QLinearConfig(\n    weights_config=MXConfig(elem_dtype_name=\"float6_e3m2\", block_size=32),\n    activations_config=MXConfig(elem_dtype_name=\"float8_e4m3\", block_size=32),\n)\n\n# Quantize the model to MXFormat. Note this quantizes the model in place\nquantize_linear_(model=model, qconfig=qconfig)\n\n\n# Perform inference using the quantized model\nx_hp = torch.randn(16, 128, dtype=torch.bfloat16)\ny_mx = model(x_hp)\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>For more detailed examples refer the examples directory</p>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#license","title":"License","text":"<p><code>torchmx</code> is released under MIT LICENSE</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find the <code>torchmx</code> library useful, please cite it in your work as below.</p> <pre><code>@software{torchmx,\n  title = {torchmx: PyTorch quantization framework for OCP MX datatypes},\n  authors = {Abhijit Balaji, Marios Fournarakis, TorchMX maintainers and contributors},\n  url = {https://github.com/rain-neuromorphics/torchmx},\n  license = {MIT License},\n  month = May,\n  year = {2025}\n}\n</code></pre>"},{"location":"dev_docs/Developer%20API%20Home/","title":"Developer API Home","text":"<p>Use this API documentation if you want to build on top of <code>TorchMX</code>. This is much more detailed and contains internal documentations.</p>"},{"location":"dev_docs/config/","title":"torchmx.config","text":""},{"location":"dev_docs/config/#torchmxconfig","title":"torchmx.config","text":""},{"location":"dev_docs/config/#mxconfig-objects","title":"MXConfig Objects","text":"<p> [view source] </p> <pre><code>@dataclass(frozen=True)\nclass MXConfig(_BaseConfig)\n</code></pre> <p>Configuration class for MX Quantization</p> <p>Arguments:</p> <ul> <li><code>elem_dtype_name</code> str - The name of the element dtype. Look at the <code>name</code>               attribute in dtypes.py for supported strings.</li> <li><code>block_size</code> int - The block size. Default 32</li> </ul> <p>Notes:</p> <p>Pass either elem_dtype or elem_dtype_name and not both.</p> <p>Methods:</p> <ul> <li><code>__post_init__()</code> - Validates the configuration parameters after initialization.</li> </ul> <p></p>"},{"location":"dev_docs/config/#elem_dtype","title":"elem_dtype","text":"<p> [view source] </p> <pre><code>@property\ndef elem_dtype() -&gt; dtypes.DType\n</code></pre> <p>Get the DType object corresponding to elem_dtype_name.</p> <p>Returns:</p> <ul> <li><code>dtypes.DType</code> - The corresponding dtypes.DType object</li> </ul> <p></p>"},{"location":"dev_docs/config/#load_from_dict","title":"load_from_dict","text":"<p> [view source] </p> <pre><code>@classmethod\ndef load_from_dict(cls, config_dict: dict) -&gt; \"MXConfig\"\n</code></pre> <p>Load the configuration from a dictionary.</p> <p>Arguments:</p> <ul> <li><code>config_dict</code> dict - The configuration dictionary.</li> </ul> <p>Returns:</p> <ul> <li><code>MXConfig</code> - The configuration object.</li> </ul> <p></p>"},{"location":"dev_docs/config/#to_dict","title":"to_dict","text":"<p> [view source] </p> <pre><code>def to_dict() -&gt; dict\n</code></pre> <p>Convert the configuration to a dictionary.</p> <p>Returns:</p> <ul> <li><code>dict</code> - The configuration dictionary.</li> </ul> <p></p>"},{"location":"dev_docs/config/#qlinearconfig-objects","title":"QLinearConfig Objects","text":"<p> [view source] </p> <pre><code>@dataclass(frozen=True)\nclass QLinearConfig(_BaseConfig)\n</code></pre> <p>Linear layer Quantization Configuration</p> <p>Arguments:</p> <ul> <li><code>weights_config</code> MXConfig - Configuration for the weights</li> <li><code>activations_config</code> MXConfig - Configuration for the activations</li> </ul> <p></p>"},{"location":"dev_docs/config/#load_from_dict_1","title":"load_from_dict","text":"<p> [view source] </p> <pre><code>@classmethod\ndef load_from_dict(cls, config_dict: dict) -&gt; \"QLinearConfig\"\n</code></pre> <p>Load the configuration from a dictionary.</p> <p>Arguments:</p> <ul> <li><code>config_dict</code> dict - The configuration dictionary.</li> </ul> <p>Returns:</p> <ul> <li><code>QLinearConfig</code> - The configuration object.</li> </ul> <p></p>"},{"location":"dev_docs/config/#to_dict_1","title":"to_dict","text":"<p> [view source] </p> <pre><code>def to_dict() -&gt; dict\n</code></pre> <p>Convert the configuration to a dictionary.</p> <p>Returns:</p> <ul> <li><code>dict</code> - The configuration dictionary.</li> </ul> <p></p>"},{"location":"dev_docs/config/#qattentionconfig-objects","title":"QAttentionConfig Objects","text":"<p> [view source] </p> <pre><code>@dataclass(frozen=True)\nclass QAttentionConfig(_BaseConfig)\n</code></pre> <p>Attention layer Quantization Configuration</p> <p>Arguments:</p> <ul> <li><code>projection_config</code> QLinearConfig - Configuration for the projection layers. Generally q,k,v,o projection layers.</li> <li><code>query_config</code> Optional[MXConfig] - Configuration for the query tensor. Default None</li> <li><code>key_config</code> Optional[MXConfig] - Configuration for the key tensor. Default None</li> <li><code>value_config</code> Optional[MXConfig] - Configuration for the value tensor. Default None</li> <li><code>attention_weights_config</code> Optional[MXConfig] - Configuration for the attention weights which is the output of torch.matmul(q,k.T) operation. Default None</li> </ul> <p></p>"},{"location":"dev_docs/config/#is_qkv_quantization_enabled","title":"is_qkv_quantization_enabled","text":"<p> [view source] </p> <pre><code>@property\ndef is_qkv_quantization_enabled() -&gt; bool\n</code></pre> <p>Check if q,k,v and attention_weights quantization is enabled.</p> <p>Returns:</p> <ul> <li><code>bool</code> - True if q,k,v and attention_weights quantization is enabled else False</li> </ul> <p></p>"},{"location":"dev_docs/config/#load_from_dict_2","title":"load_from_dict","text":"<p> [view source] </p> <pre><code>@classmethod\ndef load_from_dict(cls, config_dict: dict) -&gt; \"QAttentionConfig\"\n</code></pre> <p>Load the configuration from a dictionary.</p> <p>Arguments:</p> <ul> <li><code>config_dict</code> dict - The configuration dictionary.</li> </ul> <p>Returns:</p> <ul> <li><code>QAttentionConfig</code> - The configuration object.</li> </ul> <p></p>"},{"location":"dev_docs/config/#to_dict_2","title":"to_dict","text":"<p> [view source] </p> <pre><code>def to_dict()\n</code></pre> <p>Convert the configuration to a dictionary.</p> <p>Returns:</p> <ul> <li><code>dict</code> - The configuration dictionary.</li> </ul>"},{"location":"dev_docs/custom_float_cast/","title":"torchmx.custom_float_cast","text":""},{"location":"dev_docs/custom_float_cast/#torchmxcustom_float_cast","title":"torchmx.custom_float_cast","text":""},{"location":"dev_docs/custom_float_cast/#hp_to_floatx","title":"hp_to_floatx","text":"<p> [view source] </p> <pre><code>def hp_to_floatx(hp_data: torch.Tensor,\n                 exponent_bits: int,\n                 mantissa_bits: int,\n                 max_normal: float,\n                 round_mode=\"round_to_even\",\n                 keep_subnormals: bool = True)\n</code></pre> <p>Converts high-precision floating-point data to a custom floating-point format, as specified by the number of exponent and mantissa bits.</p> <p>Notes:</p> <ul> <li>This function does not take into account whether the data format supportes NaNs or infs.   It will return the NaNs and Infs as found in the input tesnor</li> <li>It implemention  OCP's 'saturating mode' the values to the max_normal value.</li> </ul> <p>Arguments:</p> <ul> <li><code>hp_data</code> torch.Tensor - Input tensor with high-precision floating-point data (float32 or float64).</li> <li><code>exponent_bits</code> int - Number of bits for the exponent in the target format</li> <li><code>mantissa_bits</code> int - Number of bits for the mantissa in the target format.</li> <li><code>max_normal</code> float - Maximum representable normal value in the target format.</li> <li><code>round_mode</code> str, optional - Rounding mode to use. Options are \"truncate\" and   \"round_to_even\". Default is \"round_to_even\".</li> <li><code>keep_subnormals</code> bool, optional - Whether to keep subnormal values. Default is True.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - Tensor with data converted to the custom floating-point format.</li> </ul>"},{"location":"dev_docs/dtypes/","title":"DType Constants for PyTorch MX Formats","text":""},{"location":"dev_docs/dtypes/#overview","title":"Overview","text":"<p>This module defines the <code>DType</code> class and various numerical data types used in PyTorch's MX formats. It includes information about their properties, such as exponent bias, mantissa bits, and maximum representable values.</p>"},{"location":"dev_docs/dtypes/#dtype-class-definition","title":"DType Class Definition","text":"<pre><code>@dataclass(frozen=True, repr=False)\nclass DType:\n    name: str\n    max: float  # Maximum representable value\n    max_pow2: int  # Largest power of 2 representable: floor(log2(max))\n    exponent_bias: int  # Exponent bias\n    exponent_bits: int  # Number of exponent bits\n    mantissa_bits: int  # Number of mantissa bits\n    torch_dtype: Optional[torch.dtype] = None  # Corresponding torch.dtype if available\n\n    def __repr__(self):\n        return self.name\n</code></pre>"},{"location":"dev_docs/dtypes/#supported-dtypes-for-mx-format","title":"Supported DTypes for MX Format","text":"<p>All the <code>data types</code> below are objects of the above <code>DType</code> class. You can use any of the following as input to <code>elem_dtype</code></p>"},{"location":"dev_docs/dtypes/#float-types","title":"Float Types","text":"Name Max Value Max Pow2 Exponent Bias Exponent Bits Mantissa Bits PyTorch DType <code>float8_e4m3</code> 448.0 8 7 4 3 <code>torch.float8_e4m3fn</code> <code>float6_e3m2</code> 28.0 4 3 3 2 None <code>float6_e2m3</code> 7.5 2 1 2 3 None <code>float4_e2m1</code> 6.0 2 1 2 1 None"},{"location":"dev_docs/dtypes/#integer-types","title":"Integer Types","text":"Name Max Value Max Pow2 Exponent Bias Exponent Bits Mantissa Bits PyTorch DType <code>int8</code> 127.0 6 0 0 7 <code>torch.int8</code>"},{"location":"dev_docs/dtypes/#other-convenient-variables","title":"Other convenient variables","text":""},{"location":"dev_docs/dtypes/#supported-element-types","title":"Supported Element Types","text":"<pre><code>SUPPORTED_ELEM_DTYPES = (\n    float8_e4m3,\n    float6_e3m2,\n    float6_e2m3,\n    float4_e2m1,\n    int8,\n)\n</code></pre>"},{"location":"dev_docs/dtypes/#mappings-for-easy-lookup","title":"Mappings for Easy Lookup","text":"<pre><code>STR_TO_SUPPORTED_ELEM_DTYPE = {d.name: d for d in SUPPORTED_ELEM_DTYPES}\n</code></pre>"},{"location":"dev_docs/env_variables/","title":"Environment Variables for the <code>torchmx</code> Package","text":"<p>This document describes the environment variables available for configuring the <code>torchmx</code> package.</p>"},{"location":"dev_docs/env_variables/#logging-configuration","title":"Logging Configuration","text":""},{"location":"dev_docs/env_variables/#torchmx_log_level","title":"TORCHMX_LOG_LEVEL","text":"<ul> <li>Description: Controls the verbosity level of logs.</li> <li>Default Value: <code>\"INFO\"</code></li> <li>Usage:</li> </ul> <pre><code>export TORCHMX_LOG_LEVEL=\"INFO\"\n</code></pre>"},{"location":"dev_docs/env_variables/#torchmx_log_file","title":"TORCHMX_LOG_FILE","text":"<ul> <li>Description: If set, logs from this package will be written to the specified file in addition to the console.</li> <li>Default Value: <code>None</code></li> <li>Usage:</li> </ul> <pre><code>export TORCHMX_LOG_FILE=\"/path/to/logfile.log\"\n</code></pre>"},{"location":"dev_docs/env_variables/#hardware-and-computation-settings","title":"Hardware and Computation Settings","text":""},{"location":"dev_docs/env_variables/#mx_hardware_exact_quantization","title":"MX_HARDWARE_EXACT_QUANTIZATION","text":"<ul> <li>Description: If set to <code>True</code>, hardware quantization will be performed in exact mode.</li> <li>Default Value: <code>\"False\"</code></li> <li>Usage:</li> </ul> <pre><code>export MX_HARDWARE_EXACT_QUANTIZATION=\"False\"\n</code></pre>"},{"location":"dev_docs/mx_quantization_utils/","title":"torchmx.mx_quantization_utils","text":""},{"location":"dev_docs/mx_quantization_utils/#torchmxmx_quantization_utils","title":"torchmx.mx_quantization_utils","text":""},{"location":"dev_docs/mx_quantization_utils/#unpack_bfloat16","title":"unpack_bfloat16","text":"<p> [view source] </p> <pre><code>def unpack_bfloat16(\n    x: torch.Tensor,\n    dtype: torch.dtype = torch.uint8\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Extract the sign, exponent, and mantissa from a bfloat16 tensor</p> <p>Arguments:</p> <ul> <li><code>x</code> - torch.Tensor, the input bfloat16 tensor</li> <li><code>dtype</code> - torch.dtype, the dtype to cast the output tensors to. Default is torch.uint8</li> </ul> <p>Returns:</p> <ul> <li><code>sign</code> - torch.Tensor, the sign of the tensor in uint8</li> <li><code>exponent</code> - torch.Tensor, the exponent of the tensor in uint8</li> <li><code>mantissa</code> - torch.Tensor, the mantissa of the tensor in uint8</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#unpack_fp32","title":"unpack_fp32","text":"<p> [view source] </p> <pre><code>def unpack_fp32(\n        x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Unpacks the given FP32 tensor to its components.</p> <p>Arguments:</p> <ul> <li><code>x</code> torch.Tensor - The packed FP32 tensor.</li> </ul> <p>Returns:</p> <ul> <li><code>sign</code> torch.Tensor - The sign bit tensor.</li> <li><code>exponent</code> torch.Tensor - The exponent tensor.</li> <li><code>mantissa</code> torch.Tensor - The mantissa tensor..</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#unpack_fp64","title":"unpack_fp64","text":"<p> [view source] </p> <pre><code>def unpack_fp64(\n        x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Unpacks the given FP64 tensor to its components.</p> <p>Arguments:</p> <ul> <li><code>x</code> torch.Tensor - The packed FP64 tensor.</li> </ul> <p>Returns:</p> <ul> <li><code>sign</code> torch.Tensor - The sign bit tensor.</li> <li><code>exponent</code> torch.Tensor - The exponent tensor.</li> <li><code>mantissa</code> torch.Tensor - The mantissa tensor..</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#dequantize_to_dtype","title":"dequantize_to_dtype","text":"<p> [view source] </p> <pre><code>def dequantize_to_dtype(data_lp: torch.Tensor,\n                        elem_dtype: dtypes.DType,\n                        target_dtype: torch.dtype,\n                        packing_dim: int = -1,\n                        is_packed_fp4: bool = True) -&gt; torch.Tensor\n</code></pre> <p>Dequantizes a elem_dtype packed as unit8 to the target_dtype by using an intermediate bfloa16 representation.</p> <p>Arguments:</p> <ul> <li><code>data_lp</code> torch.Tensor - The input tensor in low-precision format (must be of dtype torch.uint8).</li> <li><code>elem_dtype</code> dtypes.DType - The input element data type</li> <li><code>target_dtype</code> torch.dtype - The target data type to which the tensor will be dequantized.</li> <li><code>is_packed_fp4</code> bool, optional - A flag indicating whether the input tensor is packed in FP4   format. Defaults to True.</li> <li><code>packing_dim</code> int - The dimension along which the uint4 data is packed, default is -1.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - The dequantized tensor in the specified target data type.</li> </ul> <p>Raises:</p> <ul> <li><code>AssertionError</code> - If the element data type is not supported or if the input tensor is not of dtype torch.uint8.</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#round_to_even","title":"round_to_even","text":"<p> [view source] </p> <pre><code>def round_to_even(mantissa: torch.Tensor,\n                  mantissa_shift: torch.Tensor | int) -&gt; torch.Tensor\n</code></pre> <p>Round a mantissa to the nearest even value using a tensor of shift values.</p> <p>Arguments:</p> <p>mantissa (torch.Tensor) : A tensor containing the mantissa values to be rounded. - <code>mantissa_shift</code> torch.Tensor | int - A tensor containing the shift values   to be applied to each element of the <code>mantissa</code> tensor.   The size of the <code>mantissa_shift</code> tensor should match the size of the <code>mantissa</code> tensor.   Alternatively, a single integer value can be provided, in which case the same shift value   will be applied to all elements of the <code>mantissa</code> tensor.</p> <p>Returns:</p> <p>torch.Tensor   A tensor containing the mantissa values rounded to the nearest even value.   The size of the output tensor will match the input <code>mantissa</code> tensor.</p> <p>Notes</p> <ul> <li>The rounding follows the \"round half to even\" rule, where if the value   to be discarded is exactly halfway between two integers, the result is rounded   to the nearest even number.</li> <li>This function supports element-wise operations, where the shifting is applied   to each element of the mantissa according to the corresponding value in <code>mantissa_shift</code>.</li> </ul> <p>Examples</p> <p>mantissa = torch.tensor([0b1010011, 0b1101101], dtype=torch.int32) mantissa_shift = torch.tensor([2, 3], dtype=torch.int32) round_to_even(mantissa, mantissa_shift)   tensor([41, 27])</p> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#n_ones","title":"n_ones","text":"<p> [view source] </p> <pre><code>def n_ones(n: int) -&gt; int\n</code></pre> <p>Returns a number with n ones in binary representation. for example: _n_ones(3) = 0b111 = 7</p> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#leading_one_position","title":"leading_one_position","text":"<p> [view source] </p> <pre><code>def leading_one_position(mantissa: torch.Tensor, mantissa_size: int = 7)\n</code></pre> <p>Determine the position of the leading one bit in each element of the input tensor with LBS at position 0. If there is no 1 in the mantissa, the function returns -1.</p> <p>Arguments:</p> <ul> <li><code>mantissa</code> torch.Tensor - A tensor containing the mantissa values to be analyzed.   Each element should be an integer.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - the position of the leading one bit in each element of the input tensor.</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#quantize_mx_with_e8m0_shared_exponent_hw_exact","title":"quantize_mx_with_e8m0_shared_exponent_hw_exact","text":"<p> [view source] </p> <pre><code>def quantize_mx_with_e8m0_shared_exponent_hw_exact(\n        data_hp: torch.Tensor,\n        elem_dtype: dtypes.DType,\n        shared_exponent: torch.Tensor,\n        orig_shape: Optional[torch.Size] = None) -&gt; torch.Tensor\n</code></pre> <p>A hardware-exact MX quantization function that handles the division and conversion to that target element data type explicitly.</p> <p>Arguments:</p> <ul> <li><code>data_hp</code> torch.Tensor - The high precision input tensor, (dtype=torch.bfloat16).</li> <li><code>elem_dtype</code> dtypes.DType - The target element data type for quantization.</li> <li><code>shared_exponent</code> torch.Tensor - The E8M0 scale shared exponent (dtype=torch.uint8).</li> <li><code>orig_shape</code> torch.Size - The original shape of the input tensor, used to reshape the output   tensor. Optional, defaults to None.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> dtype=torch.uint8 - The quantized tensor in the target lower precision format.</li> </ul> <p>Raises:</p> <ul> <li><code>AssertionError</code> - If the provided elem_dtype is not supported.</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#get_fp_scale","title":"get_fp_scale","text":"<p> [view source] </p> <pre><code>def get_fp_scale(shared_exp_e8m0: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Takes the shared exponent of the MX scale, FP8(0-8-0), as a biased uint8 exponent</p> <p>Arguments:</p> <ul> <li><code>shared_exp_e8m0</code> torch.Tensor - the shared exponent of the FP8(0-8-0) scale</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - FP32 scale, 2**(shared_exponent - 127), with NaNs handling</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#quantize_mx_with_e8m0_shared_exponent_simulated","title":"quantize_mx_with_e8m0_shared_exponent_simulated","text":"<p> [view source] </p> <pre><code>def quantize_mx_with_e8m0_shared_exponent_simulated(\n        data_hp: torch.Tensor,\n        elem_dtype: dtypes.DType,\n        shared_exponent: torch.Tensor,\n        orig_shape: Optional[torch.Size] = None) -&gt; torch.Tensor\n</code></pre> <p>Simulated MX quantization function inspired by torchao. It accepts high precision input tensor (data_hp), the MX scale shared exponent (shared_exponent), and returns the quantized tensor in the elem_dtype. The steps are: 1. normalize data_hp by performing a single-precision division with an MX scale in torch.float32 2. quantize the normalized data_hp to the target elem_dtype by using the native torhcao function</p> <p>We call this implementation simulated because it is not an efficient hardware implementation</p> <p>Arguments:</p> <ul> <li><code>data_hp</code> torch.Tensor - The high precision input tensor, dtype is either torch.bfloat16 or torch.float</li> <li><code>elem_dtype</code> dtypes.DType - The target element data type for quantization.</li> <li><code>shared_exponent</code> torch.Tensor - The E8M0 scale shared exponent (dtype=torch.uint8).</li> <li><code>orig_shape</code> torch.Size - The original shape of the input tensor, used to reshape the output   tensor. Optional, defaults to None.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> dtype=torch.uint8 - The quantized tensor in the target lower precision format.</li> </ul> <p>Raises:</p> <ul> <li><code>AssertionError</code> - If the provided elem_dtype is not supported.</li> </ul> <p></p>"},{"location":"dev_docs/mx_quantization_utils/#get_e8m0_shared_exponent","title":"get_e8m0_shared_exponent","text":"<p> [view source] </p> <pre><code>def get_e8m0_shared_exponent(data_hp: torch.Tensor,\n                             elem_dtype: dtypes.DType) -&gt; torch.Tensor\n</code></pre> <p>Computes the shared exponent for a given high-precision tensor.</p> <p>Arguments:</p> <ul> <li><code>data_hp</code> torch.Tensor - High-precision input tensor, with block size as the last dimension.   dtype must be torch.bfloat16 or torch.float.</li> <li><code>elem_dtype</code> dtypes.DType - target element dtype</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - MX-scale exponent tensor as torch.uint8</li> </ul>"},{"location":"dev_docs/mx_tensor/","title":"torchmx.mx_tensor","text":""},{"location":"dev_docs/mx_tensor/#torchmxmx_tensor","title":"torchmx.mx_tensor","text":"<p>Defines the tensor subclasses to represent the OCP MX-Format spec</p> <p>Exponent E8M0 encoding details (OCP spec section 5.4.1):</p> <ul> <li>bias: 127</li> <li>supported exponent range: -127 to 127</li> <li>infinities: N/A</li> <li>NaN: 11111111</li> <li>Zeros: N/A</li> </ul> <p></p>"},{"location":"dev_docs/mx_tensor/#quantize_mx","title":"quantize_mx","text":"<p> [view source] </p> <pre><code>@torch.library.custom_op(\"torchmx::quantize_mx\", mutates_args=())\ndef quantize_mx(data_hp: torch.Tensor, elem_dtype_name: str,\n                block_size: int) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Takes a high precision tensor and converts to MX scale and raw data, in naive layout (scale and raw data are separate tensors). The function for now only supports quantization along the last dimension of the input tensor. For example, if the input tensor has shape (N, C, H, W) the output will be: - data_lp (torch.uint8) with shape (N, C, H, W) - scale (torch.uint8) with shape (N, C, H, W // block_size)</p> <p>Arguments:</p> <ul> <li><code>data_hp</code> torch.Tensor - high precision data tensor (dtype=torch.bfloat16)</li> <li><code>elem_dtype_name</code> str - target element dtype as a string to comply with torch.library.infer_schema</li> <li><code>block_size</code> int - block size</li> </ul> <p>Returns:</p> <p>Tuple[torch.Tensor, torch.Tensor]: scale(biased), low precision data as tensors</p> <p></p>"},{"location":"dev_docs/mx_tensor/#_","title":"_","text":"<p> [view source] </p> <pre><code>@quantize_mx.register_fake\ndef _(data_hp: torch.Tensor, elem_dtype_name: str,\n      block_size: int) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Fake quantize_mx implementation.</p> <p>This adds a \u201cFakeTensor kernel\u201d (aka \u201cmeta kernel\u201d) to the operator. Given some FakeTensors inputs (dummy Tensors that don't have storage), this function returns dummy Tensors with the correct Tensor metadata(shape/strides/dtype/device). This is used by torch.compile to infer the shape and other metadata of the output tensors.</p> <p>Reference: https://pytorch.org/tutorials/advanced/python_custom_ops.html#python-custom-ops-tutorial</p> <p></p>"},{"location":"dev_docs/mx_tensor/#dequantize_mx","title":"dequantize_mx","text":"<p> [view source] </p> <pre><code>@torch.library.custom_op(\"torchmx::dequantize_mx\", mutates_args=())\ndef dequantize_mx(data_lp: torch.Tensor, shared_exp_e8m0: torch.Tensor,\n                  elem_dtype_name: str, block_size: int,\n                  target_dtype: torch.dtype, block_dim: int) -&gt; torch.Tensor\n</code></pre> <p>Takes the low precision data and scale of MXTensor and converts to high precision</p> <p>Arguments:</p> <ul> <li><code>data_lp</code> torch.Tensor - low precision data tensor</li> <li><code>shared_exp_e8m0</code> torch.Tensor - biased exponent of the shared MX scale as torch.uint8</li> <li><code>elem_dtype_name</code> str - target element dtype's name as a string to comply with torch.library.infer_schema</li> <li><code>block_size</code> int - block size</li> <li><code>target_dtype</code> torch.dtype - target dtype</li> <li><code>block_dim</code> int - block dimension</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - high precision data tensor in target_dtype converted from MX</li> </ul> <p></p>"},{"location":"dev_docs/mx_tensor/#__1","title":"_","text":"<p> [view source] </p> <pre><code>@dequantize_mx.register_fake\ndef _(data_lp: torch.Tensor, shared_exp_e8m0: torch.Tensor,\n      elem_dtype_name: str, block_size: int, target_dtype: torch.dtype,\n      block_dim: int) -&gt; torch.Tensor\n</code></pre> <p>Fake dequantize_mx implementation.</p> <p>This adds a \u201cFakeTensor kernel\u201d (aka \u201cmeta kernel\u201d) to the operator. Given some FakeTensors inputs (dummy Tensors that don't have storage), this function returns dummy Tensors with the correct Tensor metadata(shape/strides/dtype/device). This is used by torch.compile to infer the shape and other metadata of the output tensors.</p> <p>Reference: https://pytorch.org/tutorials/advanced/python_custom_ops.html#python-custom-ops-tutorial</p> <p></p>"},{"location":"dev_docs/mx_tensor/#tomxconstrfunc-objects","title":"ToMXConstrFunc Objects","text":"<p> [view source] </p> <pre><code>@torch._dynamo.allow_in_graph\nclass ToMXConstrFunc(torch.autograd.Function)\n</code></pre> <p>Differentiable cast to MX, no-op in backward</p> <p></p>"},{"location":"dev_docs/mx_tensor/#forward","title":"forward","text":"<p> [view source] </p> <pre><code>@staticmethod\ndef forward(ctx, data_hp: torch.Tensor, elem_dtype: dtypes.DType,\n            block_size: int)\n</code></pre> <p>Forward method for the custom autograd function.</p> <p>Arguments:</p> <ul> <li><code>ctx</code> - The context object that can be used to stash information   for backward computation.</li> <li><code>data_hp</code> torch.Tensor - The high-precision input tensor to be quantized.</li> <li><code>elem_dtype</code> dtypes.DType - The target data type for quantization.</li> <li><code>block_size</code> int - The block size used for quantization.</li> <li><code>padding</code> int - The padding size applied during quantization.</li> </ul> <p>Returns:</p> <ul> <li><code>MXTensor</code> - A custom tensor object containing the quantized data,   scale factor, and metadata about the quantization process.</li> </ul> <p></p>"},{"location":"dev_docs/mx_tensor/#frommxconstrfunc-objects","title":"FromMXConstrFunc Objects","text":"<p> [view source] </p> <pre><code>@torch._dynamo.allow_in_graph\nclass FromMXConstrFunc(torch.autograd.Function)\n</code></pre> <p>Differentiable cast from MX, no-op in backward</p> <p></p>"},{"location":"dev_docs/mx_tensor/#forward_1","title":"forward","text":"<p> [view source] </p> <pre><code>@staticmethod\ndef forward(ctx, tensor_lp: torch.Tensor,\n            target_dtype: torch.dtype) -&gt; torch.Tensor\n</code></pre> <p>Forward method for dequantizing a low-precision tensor to a target data type.</p> <p>Arguments:</p> <ul> <li><code>ctx</code> - The context object (not used in this implementation).</li> <li><code>tensor_lp</code> torch.Tensor - The low-precision tensor to be dequantized.   This tensor is expected to have the following attributes:</li> <li>_data: The raw data of the tensor.</li> <li>_scale_e8m0: The scale factor for dequantization.</li> <li>_elem_dtype.name: The name of the element data type.</li> <li>_block_size: The block size used in the tensor.</li> <li>_block_dim: The block dimensions of the tensor.</li> <li>_padding: The amount of padding applied to the tensor.</li> <li><code>target_dtype</code> torch.dtype - The target data type to which the tensor   should be dequantized.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - The dequantized tensor, reshaped to its original shape   if padding was involved.</li> </ul> <p></p>"},{"location":"dev_docs/mx_tensor/#noopfwtomxbw-objects","title":"NoopFwToMXBw Objects","text":"<p> [view source] </p> <pre><code>@torch._dynamo.allow_in_graph\nclass NoopFwToMXBw(torch.autograd.Function)\n</code></pre> <p>Forward: no-op Backward: cast grad to MX</p> <p></p>"},{"location":"dev_docs/mx_tensor/#mxtensor-objects","title":"MXTensor Objects","text":"<p> [view source] </p> <pre><code>class MXTensor(TorchAOBaseTensor)\n</code></pre> <p></p>"},{"location":"dev_docs/mx_tensor/#__new__","title":"__new__","text":"<p> [view source] </p> <pre><code>def __new__(cls,\n            scale_e8m0_bits: torch.Tensor,\n            data_bits: torch.Tensor,\n            elem_dtype: dtypes.DType,\n            block_size: int,\n            orig_dtype: torch.dtype,\n            padding: int = 0,\n            block_dim: Optional[int] = None)\n</code></pre> <p>Create a new instance of the tensor subclass.</p> <p>Arguments:</p> <ul> <li><code>cls</code> - The class being instantiated.</li> <li><code>scale_e8m0_bits</code> torch.Tensor - A tensor containing scale factors with dtype torch.uint8.</li> <li><code>data_bits</code> torch.Tensor - A tensor containing data bits.</li> <li><code>elem_dtype</code> dtypes.DType - The element data type.</li> <li><code>block_size</code> int - The block size.</li> <li><code>orig_dtype</code> torch.dtype - The original data type.</li> <li><code>block_dim</code> int - The block dimension. Default is None. If not set it defaults to the   last dimension.</li> <li><code>padding</code> int - Padding size in case the block_dim is not multiple of the block_size   Default is 0.</li> </ul> <p>Returns:</p> <p>An instance of the tensor subclass.</p> <p>Raises:</p> <ul> <li><code>AssertionError</code> - If the dtype of scale_e8m0_bits is not torch.uint8.</li> <li><code>AssertionError</code> - If the shape of scale_e8m0_bits is not 1-dimensional.</li> <li><code>AssertionError</code> - If the dtype of data_bits is not one of the supported types.</li> <li><code>AssertionError</code> - If elem_dtype is unsupported.</li> </ul> <p></p>"},{"location":"dev_docs/mx_tensor/#to_dtype","title":"to_dtype","text":"<p> [view source] </p> <pre><code>def to_dtype(target_dtype: torch.dtype) -&gt; torch.Tensor\n</code></pre> <p>Dequantize the MXTensor to the target_dtype.</p> <p>Arguments:</p> <ul> <li><code>target_dtype</code> torch.dtype - The target data type                 (torch.bfloat16, torch.float32, etc.)                 to which the MXTensor is dequantized.</li> </ul> <p>Returns:</p> <p>The dequantized tensor in the target_dtype.</p> <p>Notes:</p> <p>The MXTensor quantization is supported only for <code>torch.bfloat16</code>. But we             allow the dequantization to either <code>torch.bfloat16</code> or <code>torch.float32</code>.   Look at the <code>quantize_mx</code> and <code>de_quantize_mx</code> functions for more details.</p> <p></p>"},{"location":"dev_docs/mx_tensor/#to_mx","title":"to_mx","text":"<p> [view source] </p> <pre><code>@staticmethod\n@torch._dynamo.allow_in_graph\ndef to_mx(data_hp: torch.Tensor,\n          elem_dtype: dtypes.DType,\n          block_size: int = 32) -&gt; \"MXTensor\"\n</code></pre> <p>Convert/Quantize a high-precision tensor to MXTensor.</p> <p>Arguments:</p> <ul> <li><code>data_hp</code> torch.Tensor - The high-precision input tensor.                   Only <code>torch.bfloat16</code> is supported. Look at the <code>quantize_mx</code>                   function for more details.</li> <li><code>elem_dtype</code> dtypes.DType - The target element data type for quantization.</li> <li><code>block_size</code> int - The block size. Default is 32.</li> </ul> <p>Returns:</p> <ul> <li><code>MXTensor</code> - The quantized tensor in the target lower precision format.</li> </ul>"},{"location":"dev_docs/ops/","title":"torchmx.ops","text":""},{"location":"dev_docs/ops/#torchmxops","title":"torchmx.ops","text":"<p>All the custom ops for the TorchMX package are defined here. This also includes the aten ops that are implemented for the MXTensor class.</p> <p></p>"},{"location":"dev_docs/ops/#mx_cast_up_op","title":"mx_cast_up_op","text":"<p> [view source] </p> <pre><code>@implements([aten.sum.dim_IntList])\ndef mx_cast_up_op(aten_op, types, args, kwargs=None)\n</code></pre> <p>Be careful with this function, this is a \"fallback\" op that casts the output of the op to the original precision. And performs the op.</p> <p>We currently need this to support the backward for admmm bias. \"addmm\" -&gt; out \"hp_gradBias\" &lt;-\"sum\" &lt;- \"identity\" &lt;- gradOut &lt;- \"hp_gradOut\"</p> <p></p>"},{"location":"dev_docs/ops/#mx_view_op","title":"mx_view_op","text":"<p> [view source] </p> <pre><code>@implements([aten.view.default])\ndef mx_view_op(aten_op, types, args, kwargs=None)\n</code></pre> <p>This is a custom op that is used to handle the view op for MXTensor. The user is not expected to call this op directly. This Op is only implemented to support some internal PyTorch functions. We only supports view op in the case following cases: - When the block dim is the last dim - This is needed for aten.linear - When the block dim is the second last dim: - The tensor must be 4D, else raises Assertion error - This is needed for the following 4D matmuls in attention: - torch.matmul(query_states, key_states.transpose(2, 3)) - torch.matmul(attn_weights, value_states)</p> <p>Raises:</p> <p>In all the other cases we raise ValueError</p> <p></p>"},{"location":"dev_docs/ops/#autocast_to_copy","title":"autocast_to_copy","text":"<p> [view source] </p> <pre><code>@implements([aten._to_copy.default])\ndef autocast_to_copy(aten_op, types, args, kwargs=None)\n</code></pre> <p>This gets called when running matmul under autocast when the input is a MXTensor, presenting as a fp32 tensor.</p>"},{"location":"dev_docs/quant_api/","title":"torchmx.quant_api","text":""},{"location":"dev_docs/quant_api/#torchmxquant_api","title":"torchmx.quant_api","text":"<p>Quantization API for LLM models.</p> <p></p>"},{"location":"dev_docs/quant_api/#mx_dynamic_activation_mx_weights","title":"mx_dynamic_activation_mx_weights","text":"<p> [view source] </p> <pre><code>def mx_dynamic_activation_mx_weights(\n        weight_elem_dtype: dtypes.DType = dtypes.float6_e3m2,\n        weight_block_size: int = 32,\n        activation_elem_dtype: dtypes.DType = dtypes.float8_e4m3,\n        activation_block_size: int = 32)\n</code></pre> <p>Quantize the model with MXFP Dynamic quantization for activations and MXFP quantization for weights. This directly replaces the nn.Linear module's weight param This is a helper function to be used with <code>torchao.quantization.quantize_</code>. You can use this if you want to quantize all Linear layers in the model with MXFP Dynamic quantization and do not want to make a distinction between Attention and MLP See below for an example of how to use this function.</p> <p>Arguments:</p> <ul> <li><code>weight_elem_dtype</code> dtypes.DType, optional - Weight element dtype. Defaults to dtypes.float6_e3m2.</li> <li><code>weight_block_size</code> int, optional - Weight block size. Defaults to 32.</li> <li><code>activation_elem_dtype</code> dtypes.DType, optional - Activation element dtype. Defaults to dtypes.float8_e4m3.</li> <li><code>activation_block_size</code> int, optional - Activation block size. Defaults to 32.</li> </ul> <p>Usage: <pre><code>import torchao\n\nmodel = LLM()\ntorchao.quantization.quantize_(\n    model,\n    mx_dynamic_activation_mx_weights(\n        weight_elem_dtype=dtypes.float6_e3m2,\n        weight_block_size=weight_block_size,\n        activation_elem_dtype=dtypes.float8_e4m3,\n        activation_block_size=activation_block_size,\n    ),\n)\nprint(f\"Quantized model: {model}\")\n</code></pre></p> <p></p>"},{"location":"dev_docs/quant_api/#quantize_linear_","title":"quantize_linear_","text":"<p> [view source] </p> <pre><code>def quantize_linear_(model: torch.nn.Module, qconfig: QLinearConfig)\n</code></pre> <p>Quantize an LLM by swapping the Linear layers in place</p> <p>This method only replaces/quantizes the linear layers. Use this as an approximation as we do not quantize QKV and other stuff. Use this only when a specific attention layer is not implemented.</p> <p>Arguments:</p> <ul> <li><code>model</code> torch.nn.Module - The model to quantize.</li> <li><code>qconfig</code> QLLMConfig - The quantization configuration.</li> </ul> <p></p>"},{"location":"dev_docs/quant_api/#quantize_llm_","title":"quantize_llm_","text":"<p> [view source] </p> <pre><code>def quantize_llm_(model: torch.nn.Module, qattention_config: QAttentionConfig,\n                  qmlp_config: QLinearConfig)\n</code></pre> <p>Quantize the LLM by swapping the Attention Layer and MLP Layer in place. The implemented Layers is expected to handle all possible quantization layers. Refer to <code>torchmx/layers/mx_llama_attention.py</code> for more details.</p> <p>Arguments:</p> <ul> <li><code>model</code> torch.nn.Module - The model to quantize.</li> <li><code>qattention_config</code> QAttentionConfig - The quantization configuration for the attention layers.</li> <li><code>qmlp_config</code> QLinearConfig - The quantization configuration for the MLP layers.</li> </ul>"},{"location":"dev_docs/utils/","title":"torchmx.utils","text":""},{"location":"dev_docs/utils/#torchmxutils","title":"torchmx.utils","text":""},{"location":"dev_docs/utils/#get_logger","title":"get_logger","text":"<p> [view source] </p> <pre><code>def get_logger(logger_name: str = \"TORCHMX\",\n               format_string:\n               str = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n               console_output: bool = True) -&gt; logging.Logger\n</code></pre> <p>Returns a logger with the specified name and format.</p> <p>Arguments:</p> <ul> <li><code>logger_name</code> str, optional - Name of the logger. Defaults to \"TORCHMX\".</li> <li><code>format_string</code> str, optional - Format of the log message. Defaults to \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\".</li> <li><code>console_output</code> bool, optional - Whether to output the logs to the console. Defaults to True.</li> </ul> <p>Returns:</p> <ul> <li><code>logging.Logger</code> - Logger with the specified name and format.</li> </ul> <p></p>"},{"location":"dev_docs/utils/#get_uniform_random_number","title":"get_uniform_random_number","text":"<p> [view source] </p> <pre><code>def get_uniform_random_number(min_val: int, max_val: int, shape: Iterable[int],\n                              dtype: torch.dtype) -&gt; torch.Tensor\n</code></pre> <p>Generate random numbers from uniform distribution of range [min_val, max_val)</p> <p>Arguments:</p> <ul> <li><code>min_val</code> int - minimum value of the range</li> <li><code>max_val</code> int - maximum value of the range</li> <li><code>shape</code> Iterable[int] - shape of the tensor</li> <li><code>dtype</code> torch.dtype - data type of the tensor</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - tensor of shape <code>shape</code> and dtype <code>dtype</code> with random numbers</li> </ul> <p></p>"},{"location":"dev_docs/utils/#tensor_size_hp_to_fp4x2","title":"tensor_size_hp_to_fp4x2","text":"<p> [view source] </p> <pre><code>def tensor_size_hp_to_fp4x2(orig_size: torch.Size,\n                            packing_dim: int) -&gt; List[int]\n</code></pre> <p>Converts the size of a tensor from half precision to fp4x2 precision.</p> <p>Arguments:</p> <ul> <li><code>orig_size</code> torch.Size - The size of the original tensor.</li> <li><code>packing_dim</code> int - The dimension where for packing 2xuint4 per byte</li> </ul> <p>Returns:</p> <ul> <li><code>List[int]</code> - The size of the tensor in fp4x2 precision.</li> </ul> <p></p>"},{"location":"dev_docs/utils/#tensor_size_fp4x2_to_hp","title":"tensor_size_fp4x2_to_hp","text":"<p> [view source] </p> <pre><code>def tensor_size_fp4x2_to_hp(orig_size: torch.Size,\n                            unpacking_dim: int) -&gt; List[int]\n</code></pre> <p>Converts the size of a tensor from fp4x2 precision to half precision by unpacking the 4-bits into 8-bits.</p> <p>Arguments:</p> <ul> <li><code>orig_size</code> torch.Size - The size of the original tensor.</li> <li><code>unpacking_dim</code> int - The dimension where for unpacking the uint4 values to a single byte</li> </ul> <p>Returns:</p> <ul> <li><code>List[int]</code> - The size of the tensor in half precision.</li> </ul> <p></p>"},{"location":"dev_docs/utils/#unpack_uint4","title":"unpack_uint4","text":"<p> [view source] </p> <pre><code>def unpack_uint4(uint8_data: torch.Tensor,\n                 packing_dim: int = -1) -&gt; torch.Tensor\n</code></pre> <p>Unpacks a tensor of uint8 values into two tensors of uint4 values.</p> <p>Arguments:</p> <ul> <li><code>uint8_data</code> torch.Tensor - A tensor containing packed uint8 values.</li> <li><code>packing_dim</code> int - The dimension along which the unpacking is performed.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - A tensor containing the unpacked uint4 values.</li> </ul> <p></p>"},{"location":"dev_docs/utils/#pack_uint4","title":"pack_uint4","text":"<p> [view source] </p> <pre><code>def pack_uint4(uint8_data: torch.Tensor,\n               packing_dim: int = -1) -&gt; torch.Tensor\n</code></pre> <p>Packs uint4 data to unit8 format along the specified dimension.</p> <p>Arguments:</p> <ul> <li><code>uint4_data</code> torch.Tensor - The input tensor containing uint8 data.</li> <li><code>packing_dim</code> int - The dimension along which to pack the data.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - A tensor with the packed uint4 data.</li> </ul> <p>Raises:</p> <ul> <li><code>AssertionError</code> - If the size of the specified dimension is not even.</li> </ul> <p>Notes:</p> <p>The function assumes that the input data is contiguous and reshapes it   accordingly. The packing is done by combining pairs of uint8 values into   a single uint8 value where each original uint8 value is treated as a uint4.</p> <p></p>"},{"location":"dev_docs/utils/#set_seed","title":"set_seed","text":"<p> [view source] </p> <pre><code>def set_seed(seed: int) -&gt; None\n</code></pre> <p>Set the random seed for reproducibility.</p> <p>Arguments:</p> <ul> <li><code>seed</code> int - The seed value to set.</li> </ul> <p>Returns:</p> <p>None</p>"},{"location":"examples/mx_matmul/","title":"MXTensor Matmul example","text":"<p>This script tests matrix multiplication operations using MXTensor from the <code>torchmx</code> library. It generates random tensors, converts them into MXTensor format, and performs a matrix multiplication on the MXTensor using <code>torch.matmul</code>.</p> <pre><code>import torch\n\nfrom torchmx import dtypes\nfrom torchmx.mx_tensor import MXTensor\nfrom torchmx.utils import get_logger, get_uniform_random_number\n\nlogger = get_logger(\"check_mxtensor_ops\")\n\n\ndef main():\n    if torch.cuda.is_available():\n        DEVICE = torch.device(\"cuda:0\")\n    else:\n        DEVICE = torch.device(\"cpu\")\n    DTYPE = torch.bfloat16\n    logger.info(f\"using device: {DEVICE}\")\n    a = get_uniform_random_number(0, 10, (128, 256), DTYPE).to(DEVICE)\n    b = get_uniform_random_number(0, 10, (256, 512), DTYPE).to(DEVICE)\n    mx_a = MXTensor.to_mx(a, elem_dtype=dtypes.float8_e4m3, block_size=32)\n    mx_b = MXTensor.to_mx(b, elem_dtype=dtypes.float8_e4m3, block_size=32)\n\n    c = torch.matmul(mx_a, mx_b)\n    logger.info(f\"matmul result shape: {c.shape}\")\n    assert isinstance(c, torch.Tensor) and not isinstance(c, MXTensor)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/quantize_linear/","title":"Quantizing Linear Layers in a model","text":"<p>TorchMX also provides tools for quantizing individual layers and modules. Here's an example of how to quantize all the linear layers in the model. The following example demonstrates how to quantize a model with torch.nn.Linear layers to MX format using the MXInferenceLinear class. In this case the weights are quantized <code>MX-fp6_e3m2</code> and the inputs to <code>MX-fp8_e4m3</code></p> <pre><code>from torch import nn\nfrom torchmx import MXTensor, dtypes\nfrom torchmx.config import QLinearConfig, MXConfig\nfrom torchmx.quant_api import quantize_linear_\n\n# Create a high-precision model\nmodel = nn.Sequential(\n          nn.Linear(128, 256),\n          nn.ReLU(),\n          nn.Linear(256, 64),\n          nn.ReLU()\n        ).to(torch.bfloat16)\n\n# Define the quantization configuration\nqconfig = QLinearConfig(\n    weights_config=MXConfig(elem_dtype_name=\"float6_e3m2\", block_size=32),\n    activations_config=MXConfig(elem_dtype_name=\"float8_e4m3\", block_size=32),\n)\n\n# Quantize the model to MXFormat. Note this quantizes the model in place\nquantize_linear_(model=model, qconfig=qconfig)\n\n\n# Perform inference using the quantized model\nx_hp = torch.randn(16, 128, dtype=torch.bfloat16)\ny_mx = model(x_hp)\n</code></pre>"},{"location":"examples/quantize_llama/","title":"Quantizing Llama Models","text":"<p>This is an end to end example which quantizes a Llama model dowloaded from HuggingFace using <code>TorchMX</code> and runs a simple chat application in the terminal</p> <pre><code>import gc\nfrom threading import Thread\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n\nfrom torchmx.config import MXConfig, QAttentionConfig, QLinearConfig\nfrom torchmx.quant_api import quantize_llm_\nfrom torchmx.utils import set_seed\n\n# This set's all the random seeds to a fixed value. For reproducibility.\nset_seed(95134)\n\n\ndef cleanup_memory() -&gt; None:\n    \"\"\"Run GC and clear GPU memory.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef main(model_name: str):\n    print(f\"Loading model {model_name}\")\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"&lt;|eot_id|&gt;\"]})\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,  # This ensures the model is loaded in BFloat16\n        attn_implementation=\"eager\",\n        device_map=device,\n    )\n    print(\"Model loaded successfully.\")\n    print(f\"Model before quantization:\\n{model}\")\n    input(\n        \"Look at the GPU memory and note it down. After quantization, it will drop! Press Enter to continue...\"\n    )\n    print(\"Quantizing model using torchmx...\")\n\n    qattention_config = QAttentionConfig(\n        projection_config=QLinearConfig(\n            weights_config=MXConfig(\n                elem_dtype_name=\"int8\",\n            ),\n            activations_config=MXConfig(\n                elem_dtype_name=\"int8\",\n            ),\n        ),\n        query_config=MXConfig(\n            elem_dtype_name=\"int8\",\n        ),\n        key_config=MXConfig(\n            elem_dtype_name=\"int8\",\n        ),\n        value_config=MXConfig(\n            elem_dtype_name=\"int8\",\n        ),\n        attention_weights_config=MXConfig(\n            elem_dtype_name=\"int8\",\n        ),\n    )\n\n    qmlp_config = QLinearConfig(\n        weights_config=MXConfig(\n            elem_dtype_name=\"int8\",\n        ),\n        activations_config=MXConfig(\n            elem_dtype_name=\"int8\",\n        ),\n    )\n    # Quantize the LLM module with torchmx. If you use quantize_linear_, it will only\n    # replace the nn.Linear layer. Thus the attention mechanism will NOT be quantized.\n    # quantize_llm_ will replace the LlamaAttention layer itself thereby quantizing the\n    # attention mechanism too If you are interested, try out quantize_linear_ as below.\n    # quantize_linear_(model=model, qconfig=qmlp_config)\n    quantize_llm_(\n        model=model, qattention_config=qattention_config, qmlp_config=qmlp_config\n    )\n    print(f\"Model after quantization:\\n{model}\")\n    print(\"Quantization complete.\")\n    print(\"Cleaning up GPU memory...\")\n    # This is needed to clean up the GPU memory after quantization.\n    cleanup_memory()\n\n    # Run inference on the model\n    model.eval()\n    print(\"Compiling model using torch.compile() ...\")\n    print(\"Model compiled successfully.\")\n    # if we do model = torch.compile(model=model, backend=rain_backend), it will\n    # run on our hardware.\n    model = torch.compile(model=model, backend=\"inductor\")\n    print(\"Starting Chat... Ctrl+C to exit the chat.\")\n    while True:\n        try:\n            user_input = input(\"\\nUser: \")\n            if user_input.lower() == \"exit\":\n                print(\"\\nExiting chat...\")\n                break\n\n            with torch.inference_mode():\n                chat_template = [\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"You are a helpful assistant. Be concise in your answers and avoid unnecessary details.\",\n                    },\n                    {\"role\": \"user\", \"content\": user_input},\n                ]\n                prompt = tokenizer.apply_chat_template(\n                    chat_template, tokenize=False, add_generation_prompt=True\n                )\n                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n                # Set up the streamer\n                streamer = TextIteratorStreamer(\n                    tokenizer, skip_prompt=True, skip_special_tokens=True\n                )\n                generation_kwargs = dict(\n                    inputs,\n                    streamer=streamer,\n                    max_new_tokens=1000,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n                thread = Thread(target=model.generate, kwargs=generation_kwargs)\n\n                thread.start()\n                print(\"\\nModel: \", end=\"\", flush=True)\n                for new_text in streamer:\n                    print(new_text, end=\"\", flush=True)\n                thread.join()\n                cleanup_memory()\n                print(\"\\n-----------------------------------------------\\n\")\n        except KeyboardInterrupt:\n            print(\"Chat ended! Exiting...\")\n            break\n\n\nif __name__ == \"__main__\":\n    # Set the model name here\n    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n    # model_name = \"meta-llama/Llama-2-7b-hf\"\n    main(model_name=model_name)\n</code></pre>"},{"location":"results/llama3.1%20results/","title":"LLaMA Quantized Inference Results","text":"<p>This section presents empirical results on the application of <code>TorchMX</code> to the LLaMA 3.1 series of models, specifically the 8B and 70B variants. Our objective is to evaluate the efficacy of quantization using the Microscaling Floating Point (MXFP) format, which allows low-bit inference across all major tensor operations. We demonstrate that TorchMX enables near-lossless inference\u2014achieving sub-2% accuracy degradation\u2014without requiring post-training calibration.</p>"},{"location":"results/llama3.1%20results/#quantization-setup","title":"Quantization Setup","text":"<p>We apply MXFP quantization with a block size of 32 to the following components:</p> <ul> <li>All weights and activations in projection and MLP layers</li> <li>Query, Key, and Value (QKV) vectors</li> <li>Attention weight matrices (used in matmul with Value)</li> </ul> <p>Matrix multiplications and softmax layers are computed in dequantized <code>bfloat16</code>.</p>"},{"location":"results/llama3.1%20results/#evaluation-setup","title":"Evaluation Setup","text":"<ul> <li>Models Evaluated: LLaMA 3.1-8B, LLaMA 3.1-70B</li> <li>Datasets: PIQ, ARC Easy, ARC Challenge, HellaSwag, Winogrande</li> <li>Baseline Precision: <code>bfloat16</code></li> <li>Inference Hardware: NVIDIA A100 80GB</li> </ul>"},{"location":"results/llama3.1%20results/#accuracy-comparison","title":"Accuracy Comparison","text":"Model ProjW ProjA MlpW MlpA Query Key Value Atten W Aver. Acc. (%) Acc. \u0394 (%) LLaMA 3.1-8B (bf16) - - - - - - - - 73.60 \u2014 FP6 FP8 FP6 FP8 - - - - 73.26 -0.34 FP6 FP6 FP6 FP6 - - - - 73.12 -0.48 FP6 FP8 FP6 FP8 FP6 FP6 FP6 FP6 71.79 -1.81 FP6 FP6 FP6 FP6 FP6 FP6 FP6 FP6 71.76 -1.84 LLaMA 3.1-70B (bf16) - - - - - - - - 79.93 \u2014 FP6 FP8 FP6 FP6 - - - - 79.35 -0.58 FP6 FP6 FP6 FP6 - - - - 78.94 -1.00 FP6 FP8 FP6 FP6 FP6 FP6 FP6 FP6 78.63 -1.30 FP6 FP6 FP6 FP6 FP6 FP6 FP6 FP6 78.63 -1.47"},{"location":"results/llama3.1%20results/#analysis-and-insights","title":"Analysis and Insights","text":"<ul> <li>Projection activations appear more sensitive to quantization than MLP activations, especially under FP6.</li> <li>A full-stack FP6 configuration achieves excellent tradeoffs, showing just \\~1.8% degradation while offering substantial compression.</li> <li>Using FP8 for activations (especially in projection) recovers up to 0.2\u20130.5% accuracy in both 8B and 70B variants.</li> <li>Value vectors can be further compressed (e.g., MXFP4) with negligible loss (results not shown).</li> </ul>"},{"location":"results/llama3.1%20results/#reproducibility","title":"Reproducibility","text":"<p>To replicate these benchmarks, see the example script:</p> <pre><code>examples/quantize_llama.md\n</code></pre>"},{"location":"user_docs/User%20API%20Home/","title":"User API Home","text":"<p>Use this documentation if you are an end user directly using <code>TorchMX</code> to quantize LLMs.</p>"},{"location":"user_docs/config/","title":"torchmx.config","text":""},{"location":"user_docs/config/#torchmxconfig","title":"torchmx.config","text":""},{"location":"user_docs/config/#mxconfig-objects","title":"MXConfig Objects","text":"<p> [view source] </p> <pre><code>@dataclass(frozen=True)\nclass MXConfig(_BaseConfig)\n</code></pre> <p>Configuration class for MX Quantization</p> <p>Arguments:</p> <ul> <li><code>elem_dtype_name</code> str - The name of the element dtype. Look at the <code>name</code>               attribute in dtypes.py for supported strings.</li> <li><code>block_size</code> int - The block size. Default 32</li> </ul> <p>Notes:</p> <p>Pass either elem_dtype or elem_dtype_name and not both.</p> <p>Methods:</p> <ul> <li><code>__post_init__()</code> - Validates the configuration parameters after initialization.</li> </ul> <p></p>"},{"location":"user_docs/config/#elem_dtype","title":"elem_dtype","text":"<p> [view source] </p> <pre><code>@property\ndef elem_dtype() -&gt; dtypes.DType\n</code></pre> <p>Get the DType object corresponding to elem_dtype_name.</p> <p>Returns:</p> <ul> <li><code>dtypes.DType</code> - The corresponding dtypes.DType object</li> </ul> <p></p>"},{"location":"user_docs/config/#load_from_dict","title":"load_from_dict","text":"<p> [view source] </p> <pre><code>@classmethod\ndef load_from_dict(cls, config_dict: dict) -&gt; \"MXConfig\"\n</code></pre> <p>Load the configuration from a dictionary.</p> <p>Arguments:</p> <ul> <li><code>config_dict</code> dict - The configuration dictionary.</li> </ul> <p>Returns:</p> <ul> <li><code>MXConfig</code> - The configuration object.</li> </ul> <p></p>"},{"location":"user_docs/config/#to_dict","title":"to_dict","text":"<p> [view source] </p> <pre><code>def to_dict() -&gt; dict\n</code></pre> <p>Convert the configuration to a dictionary.</p> <p>Returns:</p> <ul> <li><code>dict</code> - The configuration dictionary.</li> </ul> <p></p>"},{"location":"user_docs/config/#qlinearconfig-objects","title":"QLinearConfig Objects","text":"<p> [view source] </p> <pre><code>@dataclass(frozen=True)\nclass QLinearConfig(_BaseConfig)\n</code></pre> <p>Linear layer Quantization Configuration</p> <p>Arguments:</p> <ul> <li><code>weights_config</code> MXConfig - Configuration for the weights</li> <li><code>activations_config</code> MXConfig - Configuration for the activations</li> </ul> <p></p>"},{"location":"user_docs/config/#load_from_dict_1","title":"load_from_dict","text":"<p> [view source] </p> <pre><code>@classmethod\ndef load_from_dict(cls, config_dict: dict) -&gt; \"QLinearConfig\"\n</code></pre> <p>Load the configuration from a dictionary.</p> <p>Arguments:</p> <ul> <li><code>config_dict</code> dict - The configuration dictionary.</li> </ul> <p>Returns:</p> <ul> <li><code>QLinearConfig</code> - The configuration object.</li> </ul> <p></p>"},{"location":"user_docs/config/#to_dict_1","title":"to_dict","text":"<p> [view source] </p> <pre><code>def to_dict() -&gt; dict\n</code></pre> <p>Convert the configuration to a dictionary.</p> <p>Returns:</p> <ul> <li><code>dict</code> - The configuration dictionary.</li> </ul> <p></p>"},{"location":"user_docs/config/#qattentionconfig-objects","title":"QAttentionConfig Objects","text":"<p> [view source] </p> <pre><code>@dataclass(frozen=True)\nclass QAttentionConfig(_BaseConfig)\n</code></pre> <p>Attention layer Quantization Configuration</p> <p>Arguments:</p> <ul> <li><code>projection_config</code> QLinearConfig - Configuration for the projection layers. Generally q,k,v,o projection layers.</li> <li><code>query_config</code> Optional[MXConfig] - Configuration for the query tensor. Default None</li> <li><code>key_config</code> Optional[MXConfig] - Configuration for the key tensor. Default None</li> <li><code>value_config</code> Optional[MXConfig] - Configuration for the value tensor. Default None</li> <li><code>attention_weights_config</code> Optional[MXConfig] - Configuration for the attention weights which is the output of torch.matmul(q,k.T) operation. Default None</li> </ul> <p></p>"},{"location":"user_docs/config/#is_qkv_quantization_enabled","title":"is_qkv_quantization_enabled","text":"<p> [view source] </p> <pre><code>@property\ndef is_qkv_quantization_enabled() -&gt; bool\n</code></pre> <p>Check if q,k,v and attention_weights quantization is enabled.</p> <p>Returns:</p> <ul> <li><code>bool</code> - True if q,k,v and attention_weights quantization is enabled else False</li> </ul> <p></p>"},{"location":"user_docs/config/#load_from_dict_2","title":"load_from_dict","text":"<p> [view source] </p> <pre><code>@classmethod\ndef load_from_dict(cls, config_dict: dict) -&gt; \"QAttentionConfig\"\n</code></pre> <p>Load the configuration from a dictionary.</p> <p>Arguments:</p> <ul> <li><code>config_dict</code> dict - The configuration dictionary.</li> </ul> <p>Returns:</p> <ul> <li><code>QAttentionConfig</code> - The configuration object.</li> </ul> <p></p>"},{"location":"user_docs/config/#to_dict_2","title":"to_dict","text":"<p> [view source] </p> <pre><code>def to_dict()\n</code></pre> <p>Convert the configuration to a dictionary.</p> <p>Returns:</p> <ul> <li><code>dict</code> - The configuration dictionary.</li> </ul>"},{"location":"user_docs/dtypes/","title":"DType Constants for PyTorch MX Formats","text":""},{"location":"user_docs/dtypes/#overview","title":"Overview","text":"<p>This module defines the <code>DType</code> class and various numerical data types used in PyTorch's MX formats. It includes information about their properties, such as exponent bias, mantissa bits, and maximum representable values.</p>"},{"location":"user_docs/dtypes/#dtype-class-definition","title":"DType Class Definition","text":"<pre><code>@dataclass(frozen=True, repr=False)\nclass DType:\n    name: str\n    max: float  # Maximum representable value\n    max_pow2: int  # Largest power of 2 representable: floor(log2(max))\n    exponent_bias: int  # Exponent bias\n    exponent_bits: int  # Number of exponent bits\n    mantissa_bits: int  # Number of mantissa bits\n    torch_dtype: Optional[torch.dtype] = None  # Corresponding torch.dtype if available\n\n    def __repr__(self):\n        return self.name\n</code></pre>"},{"location":"user_docs/dtypes/#supported-dtypes-for-mx-format","title":"Supported DTypes for MX Format","text":"<p>All the <code>data types</code> below are objects of the above <code>DType</code> class. You can use any of the following as input to <code>elem_dtype</code></p>"},{"location":"user_docs/dtypes/#float-types","title":"Float Types","text":"Name Max Value Max Pow2 Exponent Bias Exponent Bits Mantissa Bits PyTorch DType <code>float8_e4m3</code> 448.0 8 7 4 3 <code>torch.float8_e4m3fn</code> <code>float6_e3m2</code> 28.0 4 3 3 2 None <code>float6_e2m3</code> 7.5 2 1 2 3 None <code>float4_e2m1</code> 6.0 2 1 2 1 None"},{"location":"user_docs/dtypes/#integer-types","title":"Integer Types","text":"Name Max Value Max Pow2 Exponent Bias Exponent Bits Mantissa Bits PyTorch DType <code>int8</code> 127.0 6 0 0 7 <code>torch.int8</code>"},{"location":"user_docs/dtypes/#other-convenient-variables","title":"Other convenient variables","text":""},{"location":"user_docs/dtypes/#supported-element-types","title":"Supported Element Types","text":"<pre><code>SUPPORTED_ELEM_DTYPES = (\n    float8_e4m3,\n    float6_e3m2,\n    float6_e2m3,\n    float4_e2m1,\n    int8,\n)\n</code></pre>"},{"location":"user_docs/dtypes/#mappings-for-easy-lookup","title":"Mappings for Easy Lookup","text":"<pre><code>STR_TO_SUPPORTED_ELEM_DTYPE = {d.name: d for d in SUPPORTED_ELEM_DTYPES}\n</code></pre>"},{"location":"user_docs/env_variables/","title":"Environment Variables for the <code>torchmx</code> Package","text":"<p>This document describes the environment variables available for configuring the <code>torchmx</code> package.</p>"},{"location":"user_docs/env_variables/#logging-configuration","title":"Logging Configuration","text":""},{"location":"user_docs/env_variables/#torchmx_log_level","title":"TORCHMX_LOG_LEVEL","text":"<ul> <li>Description: Controls the verbosity level of logs.</li> <li>Default Value: <code>\"INFO\"</code></li> <li>Usage:</li> </ul> <pre><code>export TORCHMX_LOG_LEVEL=\"INFO\"\n</code></pre>"},{"location":"user_docs/env_variables/#torchmx_log_file","title":"TORCHMX_LOG_FILE","text":"<ul> <li>Description: If set, logs from this package will be written to the specified file in addition to the console.</li> <li>Default Value: <code>None</code></li> <li>Usage:</li> </ul> <pre><code>export TORCHMX_LOG_FILE=\"/path/to/logfile.log\"\n</code></pre>"},{"location":"user_docs/env_variables/#hardware-and-computation-settings","title":"Hardware and Computation Settings","text":""},{"location":"user_docs/env_variables/#mx_hardware_exact_quantization","title":"MX_HARDWARE_EXACT_QUANTIZATION","text":"<ul> <li>Description: If set to <code>True</code>, hardware quantization will be performed in exact mode.</li> <li>Default Value: <code>\"False\"</code></li> <li>Usage:</li> </ul> <pre><code>export MX_HARDWARE_EXACT_QUANTIZATION=\"False\"\n</code></pre>"},{"location":"user_docs/mx_tensor/","title":"torchmx.mx_tensor","text":""},{"location":"user_docs/mx_tensor/#torchmxmx_tensor","title":"torchmx.mx_tensor","text":"<p>Defines the tensor subclasses to represent the OCP MX-Format spec</p> <p>Exponent E8M0 encoding details (OCP spec section 5.4.1):</p> <ul> <li>bias: 127</li> <li>supported exponent range: -127 to 127</li> <li>infinities: N/A</li> <li>NaN: 11111111</li> <li>Zeros: N/A</li> </ul> <p></p>"},{"location":"user_docs/mx_tensor/#mxtensor-objects","title":"MXTensor Objects","text":"<p> [view source] </p> <pre><code>class MXTensor(TorchAOBaseTensor)\n</code></pre> <p></p>"},{"location":"user_docs/mx_tensor/#to_dtype","title":"to_dtype","text":"<p> [view source] </p> <pre><code>def to_dtype(target_dtype: torch.dtype) -&gt; torch.Tensor\n</code></pre> <p>Dequantize the MXTensor to the target_dtype.</p> <p>Arguments:</p> <ul> <li><code>target_dtype</code> torch.dtype - The target data type                 (torch.bfloat16, torch.float32, etc.)                 to which the MXTensor is dequantized.</li> </ul> <p>Returns:</p> <p>The dequantized tensor in the target_dtype.</p> <p>Notes:</p> <p>The MXTensor quantization is supported only for <code>torch.bfloat16</code>. But we             allow the dequantization to either <code>torch.bfloat16</code> or <code>torch.float32</code>.   Look at the <code>quantize_mx</code> and <code>de_quantize_mx</code> functions for more details.</p> <p></p>"},{"location":"user_docs/mx_tensor/#to_mx","title":"to_mx","text":"<p> [view source] </p> <pre><code>@staticmethod\n@torch._dynamo.allow_in_graph\ndef to_mx(data_hp: torch.Tensor,\n          elem_dtype: dtypes.DType,\n          block_size: int = 32) -&gt; \"MXTensor\"\n</code></pre> <p>Convert/Quantize a high-precision tensor to MXTensor.</p> <p>Arguments:</p> <ul> <li><code>data_hp</code> torch.Tensor - The high-precision input tensor.                   Only <code>torch.bfloat16</code> is supported. Look at the <code>quantize_mx</code>                   function for more details.</li> <li><code>elem_dtype</code> dtypes.DType - The target element data type for quantization.</li> <li><code>block_size</code> int - The block size. Default is 32.</li> </ul> <p>Returns:</p> <ul> <li><code>MXTensor</code> - The quantized tensor in the target lower precision format.</li> </ul>"},{"location":"user_docs/quant_api/","title":"torchmx.quant_api","text":""},{"location":"user_docs/quant_api/#torchmxquant_api","title":"torchmx.quant_api","text":"<p>Quantization API for LLM models.</p> <p></p>"},{"location":"user_docs/quant_api/#quantize_linear_","title":"quantize_linear_","text":"<p> [view source] </p> <pre><code>def quantize_linear_(model: torch.nn.Module, qconfig: QLinearConfig)\n</code></pre> <p>Quantize an LLM by swapping the Linear layers in place</p> <p>This method only replaces/quantizes the linear layers. Use this as an approximation as we do not quantize QKV and other stuff. Use this only when a specific attention layer is not implemented.</p> <p>Arguments:</p> <ul> <li><code>model</code> torch.nn.Module - The model to quantize.</li> <li><code>qconfig</code> QLLMConfig - The quantization configuration.</li> </ul> <p></p>"},{"location":"user_docs/quant_api/#quantize_llm_","title":"quantize_llm_","text":"<p> [view source] </p> <pre><code>def quantize_llm_(model: torch.nn.Module, qattention_config: QAttentionConfig,\n                  qmlp_config: QLinearConfig)\n</code></pre> <p>Quantize the LLM by swapping the Attention Layer and MLP Layer in place. The implemented Layers is expected to handle all possible quantization layers. Refer to <code>torchmx/layers/mx_llama_attention.py</code> for more details.</p> <p>Arguments:</p> <ul> <li><code>model</code> torch.nn.Module - The model to quantize.</li> <li><code>qattention_config</code> QAttentionConfig - The quantization configuration for the attention layers.</li> <li><code>qmlp_config</code> QLinearConfig - The quantization configuration for the MLP layers.</li> </ul>"},{"location":"user_docs/utils/","title":"torchmx.utils","text":""},{"location":"user_docs/utils/#torchmxutils","title":"torchmx.utils","text":""},{"location":"user_docs/utils/#get_logger","title":"get_logger","text":"<p> [view source] </p> <pre><code>def get_logger(logger_name: str = \"TORCHMX\",\n               format_string:\n               str = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n               console_output: bool = True) -&gt; logging.Logger\n</code></pre> <p>Returns a logger with the specified name and format.</p> <p>Arguments:</p> <ul> <li><code>logger_name</code> str, optional - Name of the logger. Defaults to \"TORCHMX\".</li> <li><code>format_string</code> str, optional - Format of the log message. Defaults to \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\".</li> <li><code>console_output</code> bool, optional - Whether to output the logs to the console. Defaults to True.</li> </ul> <p>Returns:</p> <ul> <li><code>logging.Logger</code> - Logger with the specified name and format.</li> </ul> <p></p>"},{"location":"user_docs/utils/#get_uniform_random_number","title":"get_uniform_random_number","text":"<p> [view source] </p> <pre><code>def get_uniform_random_number(min_val: int, max_val: int, shape: Iterable[int],\n                              dtype: torch.dtype) -&gt; torch.Tensor\n</code></pre> <p>Generate random numbers from uniform distribution of range [min_val, max_val)</p> <p>Arguments:</p> <ul> <li><code>min_val</code> int - minimum value of the range</li> <li><code>max_val</code> int - maximum value of the range</li> <li><code>shape</code> Iterable[int] - shape of the tensor</li> <li><code>dtype</code> torch.dtype - data type of the tensor</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - tensor of shape <code>shape</code> and dtype <code>dtype</code> with random numbers</li> </ul> <p></p>"},{"location":"user_docs/utils/#tensor_size_hp_to_fp4x2","title":"tensor_size_hp_to_fp4x2","text":"<p> [view source] </p> <pre><code>def tensor_size_hp_to_fp4x2(orig_size: torch.Size,\n                            packing_dim: int) -&gt; List[int]\n</code></pre> <p>Converts the size of a tensor from half precision to fp4x2 precision.</p> <p>Arguments:</p> <ul> <li><code>orig_size</code> torch.Size - The size of the original tensor.</li> <li><code>packing_dim</code> int - The dimension where for packing 2xuint4 per byte</li> </ul> <p>Returns:</p> <ul> <li><code>List[int]</code> - The size of the tensor in fp4x2 precision.</li> </ul> <p></p>"},{"location":"user_docs/utils/#tensor_size_fp4x2_to_hp","title":"tensor_size_fp4x2_to_hp","text":"<p> [view source] </p> <pre><code>def tensor_size_fp4x2_to_hp(orig_size: torch.Size,\n                            unpacking_dim: int) -&gt; List[int]\n</code></pre> <p>Converts the size of a tensor from fp4x2 precision to half precision by unpacking the 4-bits into 8-bits.</p> <p>Arguments:</p> <ul> <li><code>orig_size</code> torch.Size - The size of the original tensor.</li> <li><code>unpacking_dim</code> int - The dimension where for unpacking the uint4 values to a single byte</li> </ul> <p>Returns:</p> <ul> <li><code>List[int]</code> - The size of the tensor in half precision.</li> </ul> <p></p>"},{"location":"user_docs/utils/#unpack_uint4","title":"unpack_uint4","text":"<p> [view source] </p> <pre><code>def unpack_uint4(uint8_data: torch.Tensor,\n                 packing_dim: int = -1) -&gt; torch.Tensor\n</code></pre> <p>Unpacks a tensor of uint8 values into two tensors of uint4 values.</p> <p>Arguments:</p> <ul> <li><code>uint8_data</code> torch.Tensor - A tensor containing packed uint8 values.</li> <li><code>packing_dim</code> int - The dimension along which the unpacking is performed.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - A tensor containing the unpacked uint4 values.</li> </ul> <p></p>"},{"location":"user_docs/utils/#pack_uint4","title":"pack_uint4","text":"<p> [view source] </p> <pre><code>def pack_uint4(uint8_data: torch.Tensor,\n               packing_dim: int = -1) -&gt; torch.Tensor\n</code></pre> <p>Packs uint4 data to unit8 format along the specified dimension.</p> <p>Arguments:</p> <ul> <li><code>uint4_data</code> torch.Tensor - The input tensor containing uint8 data.</li> <li><code>packing_dim</code> int - The dimension along which to pack the data.</li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code> - A tensor with the packed uint4 data.</li> </ul> <p>Raises:</p> <ul> <li><code>AssertionError</code> - If the size of the specified dimension is not even.</li> </ul> <p>Notes:</p> <p>The function assumes that the input data is contiguous and reshapes it   accordingly. The packing is done by combining pairs of uint8 values into   a single uint8 value where each original uint8 value is treated as a uint4.</p> <p></p>"},{"location":"user_docs/utils/#set_seed","title":"set_seed","text":"<p> [view source] </p> <pre><code>def set_seed(seed: int) -&gt; None\n</code></pre> <p>Set the random seed for reproducibility.</p> <p>Arguments:</p> <ul> <li><code>seed</code> int - The seed value to set.</li> </ul> <p>Returns:</p> <p>None</p>"}]}